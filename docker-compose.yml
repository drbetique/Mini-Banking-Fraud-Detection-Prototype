version: '3.8'

services:
  db:
    image: postgres:13-alpine
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=bankdb
    ports:
      - "5432:5432"

  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.0.1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5001:5000"
    volumes:
      - mlflow_data:/mlflow
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_BACKEND_STORE_URI=file:///mlflow/mlruns
      - MLFLOW_ARTIFACT_STORE_URI=file:///mlflow/artifacts
      - GUNICORN_CMD_ARGS=--timeout 60
    command: mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri file:///mlflow/mlruns --default-artifact-root file:///mlflow/artifacts --allowed-hosts '*'

  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    environment:
      - DATABASE_URL=postgresql://user:password@db/bankdb
      - AZURE_API_KEY=your-secret-api-key
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
    depends_on:
      - db
      - kafka
    # Ensure api also depends on prometheus to make sure it's up for scraping config
    # depends_on is not enough for scrape target discovery but for service startup order
    
  detection_service:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "8001:8001" # Expose custom metrics port
    volumes:
      - .:/app
    environment:
      - DATABASE_URL=postgresql://user:password@db/bankdb
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_TOPIC=transactions_topic
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - PROMETHEUS_PORT=8001 # Specify port for detection service metrics
    depends_on:
      - db
      - kafka
      - mlflow
    command: python detection_service.py

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus_alerts.yml:/etc/prometheus/prometheus_alerts.yml
      - prometheus_data:/prometheus
    command: --config.file=/etc/prometheus/prometheus.yml --web.enable-remote-write-receiver --web.enable-admin-api
    depends_on:
      - api # Scrapes api
      - detection_service # Scrapes detection_service

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      # Mount provisioning files for data source and dashboards
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    depends_on:
      - prometheus # Connects to prometheus

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

volumes:
  postgres_data:
  mlflow_data:
  prometheus_data:
  grafana_data:
  redis_data:
